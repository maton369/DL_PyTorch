{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e0cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dff1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1472f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7bc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c) ** 2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716d0522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adafactor',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_adafactor',\n",
       " '_functional',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f035cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ初期化（w=1.0, b=0.0）および勾配追跡の有効化\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "\n",
    "# 学習率の設定\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# PyTorchのSGD（確率的勾配降下法）オプティマイザを初期化\n",
    "# params をリストとして渡すことで最適化対象を指定\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c2b59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルによる予測値を計算（線形モデル t_p = w * t_u + b）\n",
    "t_p = model(t_u, *params)\n",
    "\n",
    "# 予測値と正解データ（摂氏）との誤差を損失関数で計算（平均二乗誤差）\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "# 損失に基づいてパラメータ（w, b）に対する勾配を自動計算\n",
    "loss.backward()\n",
    "\n",
    "# 勾配に基づいて optimizer がパラメータを更新（学習ステップ）\n",
    "optimizer.step()\n",
    "\n",
    "# 更新後のパラメータの確認\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75892429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7761, 0.1064], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パラメータの初期化（w = 1.0, b = 0.0）と勾配追跡の有効化\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "\n",
    "# 学習率を設定してSGDオプティマイザを作成（パラメータはリストで渡す）\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "# モデルによる予測値の計算（t_p = w * t_un + b）\n",
    "t_p = model(t_un, *params)\n",
    "\n",
    "# 損失関数により予測値と正解との誤差（MSE）を計算\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "# オプティマイザに保持されている既存の勾配情報を初期化（累積防止）\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 誤差に基づいてパラメータ（params）に対する勾配を自動計算\n",
    "loss.backward()\n",
    "\n",
    "# 勾配に基づいてSGDによるパラメータの更新を実行\n",
    "optimizer.step()\n",
    "\n",
    "# 更新後のパラメータを確認\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c4b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの訓練ループ関数（勾配降下法 + オプティマイザ使用）\n",
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # モデルによる予測（forward pass）\n",
    "        t_p = model(t_u, *params)\n",
    "\n",
    "        # 予測と正解の誤差（損失）を計算\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "\n",
    "        # 勾配の初期化（累積防止）\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 損失に基づいてパラメータの勾配を計算（backward pass）\n",
    "        loss.backward()\n",
    "\n",
    "        # 勾配に基づいてパラメータを更新（optimizerによるstep）\n",
    "        optimizer.step()\n",
    "\n",
    "        # 500エポックごとに現在の損失を出力\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch %d, Loss %f\" % (epoch, float(loss)))\n",
    "\n",
    "    # 最終的なパラメータを返す\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26edcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860119\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957697\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927831\n",
      "Epoch 4000, Loss 2.927680\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)  # <1>\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000, optimizer=optimizer, params=params, t_u=t_un, t_c=t_c  # <1>\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1d6d43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612898\n",
      "Epoch 1000, Loss 3.086699\n",
      "Epoch 1500, Loss 2.928580\n",
      "Epoch 2000, Loss 2.927644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate)  # <1>\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=2000, optimizer=optimizer, params=params, t_u=t_u, t_c=t_c  # <2>\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37e9d5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10,  1,  4,  5,  2,  6,  3,  8,  0]), tensor([7, 9]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]  # サンプル数（全体のデータ数）を取得\n",
    "n_val = int(0.2 * n_samples)  # 検証用に全体の20%を確保\n",
    "\n",
    "shuffled_indices = torch.randperm(\n",
    "    n_samples\n",
    ")  # データのインデックスをランダムにシャッフル\n",
    "\n",
    "# トレーニング用：最後の n_val 件を除いたインデックス\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "\n",
    "# 検証用：最後の n_val 件\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices  # 各インデックスを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d96b287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングデータと検証データに温度入力と正解値を分割\n",
    "train_t_u = t_u[train_indices]  # トレーニング用の温度（華氏）\n",
    "train_t_c = t_c[train_indices]  # トレーニング用の温度（摂氏）\n",
    "\n",
    "val_t_u = t_u[val_indices]  # 検証用の温度（華氏）\n",
    "val_t_c = t_c[val_indices]  # 検証用の温度（摂氏）\n",
    "\n",
    "# 正規化処理：入力温度を0.1倍してスケーリング\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de92328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)  # <1>\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "\n",
    "        val_t_p = model(val_t_u, *params)  # <1>\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()  # <2>\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                f\" Validation loss {val_loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f82eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 88.5971, Validation loss 43.3170\n",
      "Epoch 2, Training loss 34.4219, Validation loss 35.0349\n",
      "Epoch 3, Training loss 27.5799, Validation loss 40.2142\n",
      "Epoch 500, Training loss 9.5169, Validation loss 9.0298\n",
      "Epoch 1000, Training loss 4.5432, Validation loss 2.5969\n",
      "Epoch 1500, Training loss 3.1109, Validation loss 2.9066\n",
      "Epoch 2000, Training loss 2.6984, Validation loss 4.1562\n",
      "Epoch 2500, Training loss 2.5796, Validation loss 5.1387\n",
      "Epoch 3000, Training loss 2.5454, Validation loss 5.7558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.6473, -18.7334], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=3000,\n",
    "    optimizer=optimizer,\n",
    "    params=params,\n",
    "    train_t_u=train_t_un,  # <1>\n",
    "    val_t_u=val_t_un,  # <1>\n",
    "    train_t_c=train_t_c,\n",
    "    val_t_c=val_t_c,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca4359d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "\n",
    "        with torch.no_grad():  # <1>\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False  # <2>\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "381a3202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_forward(t_u, t_c, is_train):\n",
    "    # 学習時は勾配を有効にし、検証時は無効にする（メモリ節約と高速化）\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        t_p = model(t_u, *params)  # 予測値を計算\n",
    "        loss = loss_fn(t_p, t_c)  # 損失を計算\n",
    "    return loss  # 損失を返す"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-env)",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
