{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e0cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dff1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1472f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7bc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c) ** 2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716d0522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adafactor',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_adafactor',\n",
       " '_functional',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f035cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ初期化（w=1.0, b=0.0）および勾配追跡の有効化\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "\n",
    "# 学習率の設定\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# PyTorchのSGD（確率的勾配降下法）オプティマイザを初期化\n",
    "# params をリストとして渡すことで最適化対象を指定\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c2b59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルによる予測値を計算（線形モデル t_p = w * t_u + b）\n",
    "t_p = model(t_u, *params)\n",
    "\n",
    "# 予測値と正解データ（摂氏）との誤差を損失関数で計算（平均二乗誤差）\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "# 損失に基づいてパラメータ（w, b）に対する勾配を自動計算\n",
    "loss.backward()\n",
    "\n",
    "# 勾配に基づいて optimizer がパラメータを更新（学習ステップ）\n",
    "optimizer.step()\n",
    "\n",
    "# 更新後のパラメータの確認\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75892429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7761, 0.1064], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パラメータの初期化（w = 1.0, b = 0.0）と勾配追跡の有効化\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "\n",
    "# 学習率を設定してSGDオプティマイザを作成（パラメータはリストで渡す）\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "# モデルによる予測値の計算（t_p = w * t_un + b）\n",
    "t_p = model(t_un, *params)\n",
    "\n",
    "# 損失関数により予測値と正解との誤差（MSE）を計算\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "# オプティマイザに保持されている既存の勾配情報を初期化（累積防止）\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 誤差に基づいてパラメータ（params）に対する勾配を自動計算\n",
    "loss.backward()\n",
    "\n",
    "# 勾配に基づいてSGDによるパラメータの更新を実行\n",
    "optimizer.step()\n",
    "\n",
    "# 更新後のパラメータを確認\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c4b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの訓練ループ関数（勾配降下法 + オプティマイザ使用）\n",
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # モデルによる予測（forward pass）\n",
    "        t_p = model(t_u, *params)\n",
    "\n",
    "        # 予測と正解の誤差（損失）を計算\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "\n",
    "        # 勾配の初期化（累積防止）\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 損失に基づいてパラメータの勾配を計算（backward pass）\n",
    "        loss.backward()\n",
    "\n",
    "        # 勾配に基づいてパラメータを更新（optimizerによるstep）\n",
    "        optimizer.step()\n",
    "\n",
    "        # 500エポックごとに現在の損失を出力\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch %d, Loss %f\" % (epoch, float(loss)))\n",
    "\n",
    "    # 最終的なパラメータを返す\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26edcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860119\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957697\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927831\n",
      "Epoch 4000, Loss 2.927680\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)  # <1>\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000, optimizer=optimizer, params=params, t_u=t_un, t_c=t_c  # <1>\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1d6d43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612898\n",
      "Epoch 1000, Loss 3.086699\n",
      "Epoch 1500, Loss 2.928580\n",
      "Epoch 2000, Loss 2.927644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate)  # <1>\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=2000, optimizer=optimizer, params=params, t_u=t_u, t_c=t_c  # <2>\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9d5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-env)",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
