{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f83d920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x121801c70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3335591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d111839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# CIFAR-10の訓練用データセットを読み込み、\n",
    "# 以下の2つの前処理を適用する：\n",
    "# 1. ToTensor(): PIL画像をPyTorchのTensor形式に変換（ピクセル値を0〜1にスケーリング）\n",
    "# 2. Normalize(): 画像のRGBチャンネルごとに、訓練セット全体の平均と標準偏差を用いて正規化\n",
    "\n",
    "data_path = \"../data-unversioned/p1ch7/\"\n",
    "\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path,  # データを保存するディレクトリ\n",
    "    train=True,  # 訓練用データセットを指定\n",
    "    download=False,  # 既にダウンロード済みのため再取得しない\n",
    "    transform=transforms.Compose(\n",
    "        [  # 複数の変換を順に適用\n",
    "            transforms.ToTensor(),  # PIL → Tensor（float32, 範囲: 0〜1）\n",
    "            transforms.Normalize(  # チャンネルごとの正規化\n",
    "                (0.4915, 0.4823, 0.4468),  # 平均（R, G, B）\n",
    "                (0.2470, 0.2435, 0.2616),  # 標準偏差（R, G, B）\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85bcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a51fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10の中から \"airplane\" (label=0) と \"bird\" (label=2) のみを抽出し、\n",
    "# ラベルを {0: 0, 2: 1} に変換して2クラス分類用のデータセットを作成する。\n",
    "\n",
    "label_map = {0: 0, 2: 1}  # 元のラベルを2クラス用にマッピング\n",
    "class_names = [\"airplane\", \"bird\"]  # 使用するクラス名\n",
    "\n",
    "# 訓練データから該当クラスのみを抽出し、ラベルを変換\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\n",
    "\n",
    "# 検証データから該当クラスのみを抽出し、ラベルを変換\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c86460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_out = 2  # 出力クラス数（airplane, bird の2クラス）\n",
    "\n",
    "# CIFAR-10画像（3チャンネル, 32×32ピクセル = 3072次元）を入力とする多層パーセプトロン（MLP）モデルを構築\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),  # 入力層: 3072次元 → 中間層: 512ユニット\n",
    "    nn.Tanh(),  # 活性化関数: 双曲線正接関数（非線形変換）\n",
    "    nn.Linear(512, n_out),  # 出力層: 512ユニット → 出力クラス数（2クラス）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82651848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # ソフトマックス関数：\n",
    "    # 各要素の指数を取り、全要素の指数の合計で割ることで、\n",
    "    # 出力が確率分布（合計1になる）になるようにする\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9637faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f262197f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a714b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "# dim=1: 各行ごとにsoftmaxを適用（バッチ内の各サンプルに対して）\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]])\n",
    "\n",
    "output = softmax(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c00319",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512), nn.Tanh(), nn.Linear(512, 2), nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f41f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8628641..2.029448].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "048bace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img: CIFAR画像などのテンソル (例: shape=(3, 32, 32))\n",
    "\n",
    "# view(-1): 画像テンソルを1次元にフラット化（3×32×32 = 3072次元ベクトル）\n",
    "# unsqueeze(0): バッチ次元を追加（shape: (1, 3072)）\n",
    "img_batch = img.view(-1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cf813b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4784, 0.5216]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img_batch)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51e8c2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `out` はモデルの出力テンソル（例：shape = [バッチサイズ, クラス数]）\n",
    "# 各サンプルごとに、最もスコアの高いクラス（最大値のインデックス）を取得する\n",
    "\n",
    "_, index = torch.max(out, dim=1)\n",
    "\n",
    "# `index` は各サンプルについて、最大値（＝予測クラス）のインデックスを持つテンソル\n",
    "# 例：torch.tensor([1, 0, 2]) など（バッチ内3サンプルの予測クラスがそれぞれ1, 0, 2）\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fb0f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力画像（3×32×32 = 3072次元）を2クラスに分類する全結合ニューラルネットワークの定義\n",
    "# 活性化関数としてTanhを使用し、最後にLogSoftmaxで対数確率を出力する\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),  # 入力層: 画像のピクセル値を512次元に変換\n",
    "    nn.Tanh(),  # 非線形変換: Tanh関数で活性化（範囲：-1〜1）\n",
    "    nn.Linear(512, 2),  # 出力層: 2クラス（例：飛行機 vs 鳥）に変換\n",
    "    nn.LogSoftmax(\n",
    "        dim=1\n",
    "    ),  # 出力: 各クラスの対数確率を返す（交差エントロピー損失と組み合わせる前提）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45ff74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の定義：負の対数尤度損失（Negative Log Likelihood Loss）\n",
    "# 出力がlog-softmax形式（nn.LogSoftmax）であるモデルと一緒に使用する\n",
    "\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4be33957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5077, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "249a4ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 6.079278\n",
      "Epoch: 1, Loss: 7.598315\n",
      "Epoch: 2, Loss: 2.108224\n",
      "Epoch: 3, Loss: 10.835946\n",
      "Epoch: 4, Loss: 7.567768\n",
      "Epoch: 5, Loss: 6.629907\n",
      "Epoch: 6, Loss: 12.274743\n",
      "Epoch: 7, Loss: 4.974216\n",
      "Epoch: 8, Loss: 11.026301\n",
      "Epoch: 9, Loss: 9.617123\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512), nn.Tanh(), nn.Linear(512, 2), nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for img, label in cifar2:\n",
    "        out = model(img.view(-1).unsqueeze(0))\n",
    "        loss = loss_fn(out, torch.tensor([label]))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdc1f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの定義：学習用データ（cifar2）をミニバッチに分けて供給する\n",
    "# - batch_size=64：一度に64枚の画像を読み込む\n",
    "# - shuffle=True：毎エポックごとにデータの順番をシャッフルして汎化性能を高める\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a3d31e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.419396\n",
      "Epoch: 1, Loss: 0.353266\n",
      "Epoch: 2, Loss: 0.358240\n",
      "Epoch: 3, Loss: 0.400695\n",
      "Epoch: 4, Loss: 0.429707\n",
      "Epoch: 5, Loss: 0.471407\n",
      "Epoch: 6, Loss: 0.319822\n",
      "Epoch: 7, Loss: 0.148314\n",
      "Epoch: 8, Loss: 0.376174\n",
      "Epoch: 9, Loss: 0.364801\n",
      "Epoch: 10, Loss: 0.398969\n",
      "Epoch: 11, Loss: 0.154077\n",
      "Epoch: 12, Loss: 0.216580\n",
      "Epoch: 13, Loss: 0.263219\n",
      "Epoch: 14, Loss: 0.449941\n",
      "Epoch: 15, Loss: 0.605801\n",
      "Epoch: 16, Loss: 0.392557\n",
      "Epoch: 17, Loss: 0.449708\n",
      "Epoch: 18, Loss: 0.264848\n",
      "Epoch: 19, Loss: 0.494987\n",
      "Epoch: 20, Loss: 0.141137\n",
      "Epoch: 21, Loss: 0.246362\n",
      "Epoch: 22, Loss: 0.632205\n",
      "Epoch: 23, Loss: 0.298713\n",
      "Epoch: 24, Loss: 0.220602\n",
      "Epoch: 25, Loss: 0.292281\n",
      "Epoch: 26, Loss: 0.315676\n",
      "Epoch: 27, Loss: 0.226785\n",
      "Epoch: 28, Loss: 0.199365\n",
      "Epoch: 29, Loss: 0.269055\n",
      "Epoch: 30, Loss: 0.054453\n",
      "Epoch: 31, Loss: 0.206067\n",
      "Epoch: 32, Loss: 0.252586\n",
      "Epoch: 33, Loss: 0.125446\n",
      "Epoch: 34, Loss: 0.196721\n",
      "Epoch: 35, Loss: 0.182928\n",
      "Epoch: 36, Loss: 0.114497\n",
      "Epoch: 37, Loss: 0.150746\n",
      "Epoch: 38, Loss: 0.210336\n",
      "Epoch: 39, Loss: 0.090225\n",
      "Epoch: 40, Loss: 0.135327\n",
      "Epoch: 41, Loss: 0.243099\n",
      "Epoch: 42, Loss: 0.078004\n",
      "Epoch: 43, Loss: 0.096502\n",
      "Epoch: 44, Loss: 0.074525\n",
      "Epoch: 45, Loss: 0.120142\n",
      "Epoch: 46, Loss: 0.119961\n",
      "Epoch: 47, Loss: 0.158445\n",
      "Epoch: 48, Loss: 0.041873\n",
      "Epoch: 49, Loss: 0.029363\n",
      "Epoch: 50, Loss: 0.109245\n",
      "Epoch: 51, Loss: 0.028567\n",
      "Epoch: 52, Loss: 0.089713\n",
      "Epoch: 53, Loss: 0.080910\n",
      "Epoch: 54, Loss: 0.132659\n",
      "Epoch: 55, Loss: 0.038777\n",
      "Epoch: 56, Loss: 0.047422\n",
      "Epoch: 57, Loss: 0.057227\n",
      "Epoch: 58, Loss: 0.095332\n",
      "Epoch: 59, Loss: 0.050806\n",
      "Epoch: 60, Loss: 0.090432\n",
      "Epoch: 61, Loss: 0.027817\n",
      "Epoch: 62, Loss: 0.104231\n",
      "Epoch: 63, Loss: 0.058049\n",
      "Epoch: 64, Loss: 0.084218\n",
      "Epoch: 65, Loss: 0.107187\n",
      "Epoch: 66, Loss: 0.049083\n",
      "Epoch: 67, Loss: 0.079775\n",
      "Epoch: 68, Loss: 0.044667\n",
      "Epoch: 69, Loss: 0.016554\n",
      "Epoch: 70, Loss: 0.071466\n",
      "Epoch: 71, Loss: 0.082602\n",
      "Epoch: 72, Loss: 0.021504\n",
      "Epoch: 73, Loss: 0.043905\n",
      "Epoch: 74, Loss: 0.035640\n",
      "Epoch: 75, Loss: 0.081018\n",
      "Epoch: 76, Loss: 0.013851\n",
      "Epoch: 77, Loss: 0.043245\n",
      "Epoch: 78, Loss: 0.027304\n",
      "Epoch: 79, Loss: 0.059829\n",
      "Epoch: 80, Loss: 0.027895\n",
      "Epoch: 81, Loss: 0.079634\n",
      "Epoch: 82, Loss: 0.024405\n",
      "Epoch: 83, Loss: 0.023356\n",
      "Epoch: 84, Loss: 0.034144\n",
      "Epoch: 85, Loss: 0.034326\n",
      "Epoch: 86, Loss: 0.014500\n",
      "Epoch: 87, Loss: 0.010436\n",
      "Epoch: 88, Loss: 0.034524\n",
      "Epoch: 89, Loss: 0.041588\n",
      "Epoch: 90, Loss: 0.017321\n",
      "Epoch: 91, Loss: 0.020087\n",
      "Epoch: 92, Loss: 0.040375\n",
      "Epoch: 93, Loss: 0.010160\n",
      "Epoch: 94, Loss: 0.042795\n",
      "Epoch: 95, Loss: 0.020476\n",
      "Epoch: 96, Loss: 0.055830\n",
      "Epoch: 97, Loss: 0.013103\n",
      "Epoch: 98, Loss: 0.022424\n",
      "Epoch: 99, Loss: 0.020298\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-2データセットの2クラス分類モデルの学習ループ\n",
    "# 入力：3x32x32画像を1次元（3072次元）にflattenして全結合ネットワークに通す\n",
    "# ネットワーク構造：Linear(3072→128) → Tanh → Linear(128→2) → LogSoftmax\n",
    "# 最適化手法：確率的勾配降下法（SGD）、損失関数：負の対数尤度損失（NLLLoss）\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 128),  # 入力層（3072次元）から中間層（128次元）\n",
    "    nn.Tanh(),  # 活性化関数（双曲線正接）\n",
    "    nn.Linear(128, 2),  # 出力層（2クラス分類）\n",
    "    nn.LogSoftmax(dim=1),  # 出力を対数ソフトマックスに変換（NLLLossに対応）\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "n_epochs = 100\n",
    "\n",
    "# 学習ループ\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        # 画像を1次元（バッチサイズ x 3072）にflatten\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))  # 順伝播\n",
    "        loss = loss_fn(outputs, labels)  # 損失計算\n",
    "\n",
    "        optimizer.zero_grad()  # 勾配初期化\n",
    "        loss.backward()  # 勾配計算（逆伝播）\n",
    "        optimizer.step()  # パラメータ更新\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))  # 各エポック終了時に損失表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "336b403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.512133\n",
      "Epoch: 1, Loss: 0.555352\n",
      "Epoch: 2, Loss: 0.436777\n",
      "Epoch: 3, Loss: 0.351507\n",
      "Epoch: 4, Loss: 0.277133\n",
      "Epoch: 5, Loss: 0.523798\n",
      "Epoch: 6, Loss: 0.413200\n",
      "Epoch: 7, Loss: 0.392049\n",
      "Epoch: 8, Loss: 0.502444\n",
      "Epoch: 9, Loss: 0.569020\n",
      "Epoch: 10, Loss: 0.437988\n",
      "Epoch: 11, Loss: 0.464160\n",
      "Epoch: 12, Loss: 0.403606\n",
      "Epoch: 13, Loss: 0.366443\n",
      "Epoch: 14, Loss: 0.258983\n",
      "Epoch: 15, Loss: 0.443610\n",
      "Epoch: 16, Loss: 0.270309\n",
      "Epoch: 17, Loss: 0.460021\n",
      "Epoch: 18, Loss: 0.198564\n",
      "Epoch: 19, Loss: 0.424789\n",
      "Epoch: 20, Loss: 0.199077\n",
      "Epoch: 21, Loss: 0.398109\n",
      "Epoch: 22, Loss: 0.270855\n",
      "Epoch: 23, Loss: 0.321322\n",
      "Epoch: 24, Loss: 0.225489\n",
      "Epoch: 25, Loss: 0.276578\n",
      "Epoch: 26, Loss: 0.193953\n",
      "Epoch: 27, Loss: 0.135430\n",
      "Epoch: 28, Loss: 0.444134\n",
      "Epoch: 29, Loss: 0.174300\n",
      "Epoch: 30, Loss: 0.161188\n",
      "Epoch: 31, Loss: 0.225456\n",
      "Epoch: 32, Loss: 0.298732\n",
      "Epoch: 33, Loss: 0.317133\n",
      "Epoch: 34, Loss: 0.173825\n",
      "Epoch: 35, Loss: 0.191741\n",
      "Epoch: 36, Loss: 0.087191\n",
      "Epoch: 37, Loss: 0.125472\n",
      "Epoch: 38, Loss: 0.257121\n",
      "Epoch: 39, Loss: 0.043465\n",
      "Epoch: 40, Loss: 0.196362\n",
      "Epoch: 41, Loss: 0.100259\n",
      "Epoch: 42, Loss: 0.104249\n",
      "Epoch: 43, Loss: 0.299599\n",
      "Epoch: 44, Loss: 0.091202\n",
      "Epoch: 45, Loss: 0.145911\n",
      "Epoch: 46, Loss: 0.268954\n",
      "Epoch: 47, Loss: 0.048175\n",
      "Epoch: 48, Loss: 0.259206\n",
      "Epoch: 49, Loss: 0.089254\n",
      "Epoch: 50, Loss: 0.077463\n",
      "Epoch: 51, Loss: 0.040271\n",
      "Epoch: 52, Loss: 0.081585\n",
      "Epoch: 53, Loss: 0.098959\n",
      "Epoch: 54, Loss: 0.072516\n",
      "Epoch: 55, Loss: 0.068711\n",
      "Epoch: 56, Loss: 0.098366\n",
      "Epoch: 57, Loss: 0.024515\n",
      "Epoch: 58, Loss: 0.074090\n",
      "Epoch: 59, Loss: 0.136832\n",
      "Epoch: 60, Loss: 0.032032\n",
      "Epoch: 61, Loss: 0.012830\n",
      "Epoch: 62, Loss: 0.041424\n",
      "Epoch: 63, Loss: 0.052375\n",
      "Epoch: 64, Loss: 0.028968\n",
      "Epoch: 65, Loss: 0.034784\n",
      "Epoch: 66, Loss: 0.039455\n",
      "Epoch: 67, Loss: 0.053474\n",
      "Epoch: 68, Loss: 0.014809\n",
      "Epoch: 69, Loss: 0.031650\n",
      "Epoch: 70, Loss: 0.034508\n",
      "Epoch: 71, Loss: 0.033960\n",
      "Epoch: 72, Loss: 0.024942\n",
      "Epoch: 73, Loss: 0.051989\n",
      "Epoch: 74, Loss: 0.007707\n",
      "Epoch: 75, Loss: 0.057815\n",
      "Epoch: 76, Loss: 0.050221\n",
      "Epoch: 77, Loss: 0.018923\n",
      "Epoch: 78, Loss: 0.014137\n",
      "Epoch: 79, Loss: 0.007485\n",
      "Epoch: 80, Loss: 0.063251\n",
      "Epoch: 81, Loss: 0.048261\n",
      "Epoch: 82, Loss: 0.035137\n",
      "Epoch: 83, Loss: 0.037646\n",
      "Epoch: 84, Loss: 0.023419\n",
      "Epoch: 85, Loss: 0.026348\n",
      "Epoch: 86, Loss: 0.011457\n",
      "Epoch: 87, Loss: 0.028547\n",
      "Epoch: 88, Loss: 0.022950\n",
      "Epoch: 89, Loss: 0.025783\n",
      "Epoch: 90, Loss: 0.022340\n",
      "Epoch: 91, Loss: 0.013042\n",
      "Epoch: 92, Loss: 0.005815\n",
      "Epoch: 93, Loss: 0.032828\n",
      "Epoch: 94, Loss: 0.040715\n",
      "Epoch: 95, Loss: 0.008702\n",
      "Epoch: 96, Loss: 0.011788\n",
      "Epoch: 97, Loss: 0.018336\n",
      "Epoch: 98, Loss: 0.010726\n",
      "Epoch: 99, Loss: 0.013249\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-2分類モデルの訓練スクリプト\n",
    "# - モデル：全結合ニューラルネットワーク（2層）\n",
    "# - 活性化関数：Tanh\n",
    "# - 出力：2クラス用のLogSoftmax\n",
    "# - 損失関数：負の対数尤度損失（NLLLoss）\n",
    "# - 最適化手法：SGD（学習率 0.01）\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),  # 入力画像（3x32x32 = 3072）→中間層（512）\n",
    "    nn.Tanh(),  # Tanh活性化関数\n",
    "    nn.Linear(512, 2),  # 中間層→出力層（2クラス）\n",
    "    nn.LogSoftmax(dim=1),  # 出力に対してLogSoftmaxを適用\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)  # SGDによる最適化\n",
    "loss_fn = nn.NLLLoss()  # 負の対数尤度損失（LogSoftmaxに対応）\n",
    "n_epochs = 100\n",
    "\n",
    "# 学習ループ（100エポック）\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))  # 画像をflattenして入力\n",
    "        loss = loss_fn(outputs, labels)  # 損失を計算\n",
    "\n",
    "        optimizer.zero_grad()  # 勾配を初期化\n",
    "        loss.backward()  # 誤差逆伝播による勾配計算\n",
    "        optimizer.step()  # パラメータを更新\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))  # エポックごとの損失を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce1d0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.999500\n"
     ]
    }
   ],
   "source": [
    "# 学習済みモデルによる訓練データ上での分類精度評価\n",
    "# - 評価時は勾配を追跡しないよう torch.no_grad() を使用\n",
    "# - DataLoader の shuffle=False により順序を保ったまま評価\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0  # 正解数のカウント\n",
    "total = 0  # 総サンプル数\n",
    "\n",
    "with torch.no_grad():  # 評価中は勾配計算を無効化\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))  # 入力画像をflattenして予測\n",
    "        _, predicted = torch.max(outputs, dim=1)  # 出力最大のクラスを予測値とする\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())  # 正解との一致数を加算\n",
    "\n",
    "print(\"Accuracy: %f\" % (correct / total))  # 精度（Accuracy）を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee9491ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.817500\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "\n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c596f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bba88bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "176457e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.364529\n",
      "Epoch: 1, Loss: 0.607475\n",
      "Epoch: 2, Loss: 0.274583\n",
      "Epoch: 3, Loss: 0.461011\n",
      "Epoch: 4, Loss: 0.447233\n",
      "Epoch: 5, Loss: 0.593781\n",
      "Epoch: 6, Loss: 0.311108\n",
      "Epoch: 7, Loss: 0.437446\n",
      "Epoch: 8, Loss: 0.278297\n",
      "Epoch: 9, Loss: 0.240712\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2944240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.803600\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "\n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9771183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.789500\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "\n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4cfabc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-env)",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
